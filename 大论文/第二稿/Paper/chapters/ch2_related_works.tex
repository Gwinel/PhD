\chapter{并发哈希表的相关研究}

% 引子

\section{哈希表概述}

\subsection{相关概念}

哈希表、树、链表等都属于搜索数据结构。
搜索数据结构由元素集合以及访问和操作这些元素的接口构成。
如果搜索数据结构能够被多个处理器共享，我们则称该数据结构为\textbf{并发搜索数据结构}(CSDS)。
哈希表（hash table），又名散列表，是一种应用广泛的搜索数据结构，它通过键值对（key-value）实现对关联数据的高效存取。
键值对之间的映射关系称为\textbf{哈希函数}。
一般的哈希表都提供了\textit{add(),remove()}和\textit{find()}三种操作的接口。
哈希表的操作分为\textbf{读}操作和\textbf{写}操作，其中读操作指哈希表的查询操作，写操作包括在哈希表中插入和删除元素。
哈希表和树型数据结构相比的最大的优势是哈希表的查询复杂度可以到常数级。
存放值的存储空间称为\textbf{哈希桶}（bucket）或者\textbf{哈希槽}（slot）。
哈希表中存放的元素的数量与哈希桶数量的比值称为\textbf{负载因子}（load factor)。
哈希表使用哈希函数计算得到一个索引值，该索引值表明键对应的值在桶数组中的位置。
关于哈希函数，有一个最理想的原则：将每一个key映射到单独的哈希桶内。
但是，当数据集规模很大时，能够完美的践行上述原则的哈希函数并不存在。
因此在实际的映射过程中往往会出现多个key对应相同的索引值，这时称为发生了\textbf{碰撞}（collision）。
既然碰撞无法避免，那么我们能做的就是在设计哈希表的时候尽量的选择好的哈希函数。
一个好哈希函数的基本需求是输出的哈希值比较均匀。
这样可以使发生碰撞的概率最小化，同时使得各个bucket中碰撞的条目比较平均。
有国外的研究人员对已有的哈希函数做过比较~\cite{Josh2012}，结论是MurmurHash3~\cite{Murmurhash}~和CityHash~\cite{cityhash}~是迄今为止最出色的哈希函数。

\subsection{哈希函数}
哈希函数是将任意大小的数据转换成特定大小的数据的函数，转换后的数据称为哈希值或哈希编码。
哈希函数是实现哈希表和布隆过滤器的基础。
根据其应用场景可以划分成加密和非加密两类。
非加密的哈希函数通过数学运算将字符串转化成整型数输出。
哈希函数的一个重要特点是它的输出能够在可能的输出域内尽量的保证均匀分布，尤其是当具有比较相近的输入时，这种特性尤为可贵。
与加密哈希函数所不同的是，非加密哈希函数无法承担阻止攻击者利用碰撞进行攻击的任务。
非加密哈希函数的运算速度要比加密哈希函数快。
哈希表通常采用非加密哈希函数建立元素与哈希表的对应关系。

Bob Jenkins长期从事哈希函数的研究，他在1997年对哈希函数的研究中提出了被后来研究人员称为Jenkins的哈希函数\cite{jenkins1997hash}，在接下来的研究中，他对其研究成果进行了扩展，提出了名为lookup2和lookup3的哈希函数\cite{jenkins2006function}。lookup3哈希函数被有关学者认为是第一款“现代的”哈希函数。
2008年，Austin Appleby发布了名为MurmurHash的哈希函数\cite{Murmurhash}。
最新的Murmurhash具有两倍于lookup3的性能。由于其卓越的运算速度和统计特性，MurmurHash得到广泛应用。
2011年，发布了两款高性能的哈希函数：一款是Google发布的CityHash\cite{cityhash}；另一款是由Jenkins提出的SpookyHash\cite{jenkins2012spookyhash}。这两款哈希函数都是基于MurmurHash，其性能的提升在很大程度上得益于更高的指令集并行。
这两款哈希函数都有两倍于MurmurHash的处理速度，CityHash的速度源于SSE 4.2中的CRC32指令。
SpookyHash产生128的输出结果，而CityHash的输出结果更为灵活，可生成64位、128位和256位的哈希值。

\subsection{哈希冲突处理}

处理碰撞的方法大致可以分为两类：一类是\textbf{开放寻址法}(open-addressing)；一类是\textbf{开链法}(separate chaining)。

开放寻址法，所有元素都存放在哈希桶数组内，当需要在哈希表中插入新元素时，将对哈希桶进行扫描，从被直接映射到的哈希桶开始，按照某种探测序列进行扫描，知道找到空闲的哈希桶为止。
当需要查找某个元素时，需要以同样的探测序列进行查找，直到找到所需的元素，或者最终发现元素不在表中为止。
常用于开放寻址法的探测序列有线性探测、二次探测、双重哈希以及Cuckoo哈希。

\textbf{线性探测}。
在线性探测中，进行探测的初始位置由$h(k)$确定，地址增量为$i$，从当前位置开始，若为空，则插入元素，若非空，则探测距离当前位置$i$个单位的位置继续，直到找到空闲的位置或者便利完整个哈希表为止。
为了确保能够遍历到整个哈希表，$i$通常是哈希表容量$m$的相对质数。
利用线性探测计算key的位置的公式如式~\ref{equ:linear_prob}所示:
\begin{equation}
	h(k,i) = (h(k)+i) ~{mod}~m
\label{equ:linear_prob}
\end{equation}
当地址增量$i$等于1时，探测的位置是连续的。
线性探测的性能取决于查找key的位置时进行的探测次数，而探测次数又取决于哈希表的负载因子。
哈希表的负载因子越高，查找一个key的位置时所需的探测次数就越多。
Knuth的论证表明\cite{knuth1998art}，使用线性探测法完成一次key的位置探测所需的平均探测次数约为$1/2(1+\frac{1}{1-\alpha})$。


\textbf{二次探测}与线性探测的相关性很高。
二次探测计算key的位置的公式如式~\ref{equ:quad_prob}所示：
\begin{equation}
h(k,i) = (h(k)+c_1 i+c_2i^2)~{mod}~m
\label{equ:quad_prob}
\end{equation}
式中$c_1$和$c_2$均为常数。
Heileman等人的研究表明，当哈希表的大小超过缓存的容量时，使用线性探测的性能要好于使用二次探测\cite{heileman2005caching}。
虽然线性探测通常需要比二次探测执行更多的探测尝试，但是这些尝试具有更高的缓存命中率，从而在整个哈希表的大小超过了缓存容量时表现出更好的整体性能。
另一方面，当哈希表的负载因子较高时，使用二次探测能够获得更好的性能，当然这只是相对而言的，当$\alpha$接近1时，使用二次探测的性能也不理想\cite{}。
同样Knuth对使用二次探测完成一次key的位置的探测所需的平均探测次数进行了估计，约为$1-ln(1-\alpha)-\frac{\alpha}{2}$。

\textbf{双重哈希}，顾名思义就是使用两个哈希函数$h_1$和$h_2$计算需要探测的初始索引值。
计算公式如式~\ref{equ:double}所示：
\begin{equation}
h(k,i) = (h_1(k)+i\cdot h_2(k)) ~{mod}~m
\label{equ:double}
\end{equation}
由于$h_2$提供可变的地址增量，所以双重哈希很好的解决了线性探测和二次探测中的“聚集”问题。
此外，多重碰撞的设置产生比线性或二次探测更为均匀的键值分布。
双重哈希方法的性能也会随着哈希表负载因子的增加而下降。
使用双重哈希完成一次查找所需的平均探测次数为$-\frac{1}{\alpha}ln(1-\alpha)$。

\textbf{Cuckoo哈希}
Cuckoo哈希的特性是当在哈希表中插入新的key时，会将原来存储与该位置上的key排挤到其他的位置上去。
一般设置两个哈希函数$h_1(k)$和$h_2(k)$。
当插入新的key时，可以任意的选择在$h_1(k)$和$h_2(k)$位置上进行插入。
如果插入的位置上已经存有了$k_1$，则将$k$覆盖$k_1$，然后将$k_1$存入到$h_1(k)$和$h_2(k)$中空闲的位置。
如果$h_1(k)$和$h_2(k)$中都没有空闲位置，则继续上述过程，直到所有的key都找到了合适的位置或者执行替换的次数达到上限\cite{pagh2004cuckoo,erlingsson2006cool}。
Pagh等人的研究表明，要保持Cuckoo哈希的最优性能，需要确保$\alpha\leq 0.5$\cite{pagh2004cuckoo}；Erlingsson等人提出了一种广义的Cuckoo哈希技术\cite{erlingsson2006cool}，能够使哈希表的负载因子达到99\%时仍然具有较好的性能；
Ross等人通过应用单指令多数据流(SIMD)指令挖掘并行性并消除探测函数内的分支指令实现了对广义Cuckoo哈希方法的性能优化\cite{ross2007efficient}。

上述的几种常用的开放寻址法中，性能都与哈希表的负载因子相关。
负载因子越高，完成一次查找所需的探测次数就越多，性能也随之受到影响。
实际上，即便是再好的哈希函数，在负载因子大于0.7之后性能会急剧下降。

另一类常用的冲突处理方法称为开链法。
开链法引入额外的数据结构，比如链表，用于解决哈希冲突问题。
每一个哈希桶都是独立的，所有经过哈希之后具有相同索引值的元素都放在同一个哈希桶中，这些元素通过链表进行管理。
所以，使用开链法的哈希表能够存储的元素的个数大于哈希桶的数目，也就是说它的负载因子可以大于1。
对哈希表执行操作的时间等于找到相应的哈希桶的时间加上对列表进行操作的时间。
虽说使用开链法的哈希表的负载因子可以大于1，并不意味着链入同一个哈希桶中的元素的个数可以是无限制的，如果某一个位置冲突过多的话，插入的时间复杂度将退化为O(N)，这种退化将引起缓存未命中率的骤然升高\cite{black1998graph}。
因此，每个哈希桶内元素的个数应在3个以内。

两种方法各有优劣，开放寻址在解决当前冲突的情况下可能会导致新的冲突，而开链不会产生这种问题。另一方面开链的局部性较之开放寻址法要差，在程序运行过程中可能引起操作系统的缺页中断，从而导致系统颠簸。

哈希表被广泛实现系统层和应用层软件，被集成到编程语言如Java，Python等，还可以用来实现关联数组，数据库索引，缓存，集合等。
哈希表的高效性使其具有重要的研究价值和应用价值。

\section{基于软件技术的同步方法研究}

在并发哈希的设计中，除了选用的哈希函数、所采用的冲突处理方案以及数据结构上的差异造成性能上的差别之外，另一个重要的因素是选取的同步方案。
并发数据结构的设计中，常用的同步方法大致可以划分成三类：锁方法、内存屏障技术和事务内存。

\subsection{阻塞技术}
\subsubsection{锁方法}
在并发数据结构中锁用于保证多个线程对数据结构的互斥访问，以避免线程间发生“错误的”交错，从而产生预期之外的结果。
设计锁算法的关键问题是当线程$t_1$试图申请线程$t_2$已经占有的锁时，$t_1$要采取的行动。
在单处理器系统上，出现这种情况时唯一明智的处理方式是将处理器让给$t_1$即可。
但是在多处理器系统上，因为锁可能在不长的时间内会被另一个处理器上执行的线程释放，所以使$t_1$反复尝试获取锁有助于提升性能。
这种使线程不断的尝试获取锁的技术称为\textbf{自旋锁(spinlock)}。
在线程执行期间很难预测该线程会持有锁多长时间，所以很难在阻塞技术和自旋锁之间做出选择。
如果操作系统直接支持锁，诸如当前持有锁的线程等信息能够用于做出抉择。

简单的自旋锁重复使用同步原语，如比较和交换(swap-and-change，CAS)，以原子方式将锁从无主状态转换到被占有状态。 如果锁的设计不够仔细，自旋锁会引起激烈的锁竞争，从而对性能造成严重影响。
一种简单的降低锁竞争的方法是引入指数退避(exponential backoff)机制\cite{agarwal1989adaptive}。
使用这种方法获取锁失败的线程在进行重试之前会等待一段时间；
失败的次数越多，等待的时间越长，此时线程会及时的“自行分散”，由此降低了线程对锁的争用，同时也减少了由于尝试获取锁失败引起的流量开销。

使用指数退避机制的锁的缺陷是锁可以处于无主状态，而尝试获取该锁的线程都已经执行退避策略，需要等待较长的时间，因此在这段时间内所有的线程都不会前向执行。
解决这个问题方法是使所有申请锁的线程存储在一个队列中，锁被释放后将锁的所有权传递给下一个正在排队的线程。
基于这种方法实现的锁被称为\textbf{队列锁(quenelocks)}。
Anderson\cite{anderson1989performance}和Graunke\cite{graunke1990synchronization}提出了基于阵列的队列锁方法。
之后M.Crummey和T.Scott\cite{mellor1991algorithms}对他们的方法进行改进实现了的基于列表的MCS队列锁，以及由Craig和E.Hagersten等人\cite{craig1993building,magnusson1994queue}提出的CLH队列锁。

使用CLH锁的线程形成一个虚拟的节点链表，每个节点都包含一个$done$标志；某个线程只有当列表中它的前继节点的$done$标志被触发后才进入临界区。
为了获得锁，线程创建一个节点，将它的$done$标志设置为$false$，表示它还没有释放临界区，并且使用同步原语(如CAS)将它自己的节点放在列表的尾部，同时确定其前继节点。
随后，该线程在其前继节点的$done$标志上自旋。
值得一提的是，每个线程的自旋过程发生在不同的内存位置，因此，在基于缓存的体系结构中，当某个线程设置其$done$标志以通知队列中的下一个线程可以进入临界区时，所有其他正在自旋的线程的$done$标志不会被修改，这些线程将继续 在本地缓存行上自旋，而不会产生额外的内存流量。
这在很大程度上减少了争用，提升了扩展性。
但是，如果使用这种锁算法的程序运行在非一致性的NUMA平台上，某些线程不得不在远程内存结点上进行自旋，这样无疑会消耗更多的内存流量。
使用MCS队列锁\cite{mellor1991algorithms}通过将线程自旋的位置限定在该线程自身结点的$done$标志来解决NUMA平台上的问题。

此外，为了迎合特定的数据结构的数据读取特性在后续的演化中出现了一些标准锁方法的变体。
队列锁算法中出现了一种具有“可中止的”特性的版本，它允许正在申请锁的线程放弃等待，比如在实时性要求较高的应用中延迟超过极限值时\cite{scott2002non,scott2001scalable}，或者线程需要从死锁中进行恢复时。
M.Scott等人提出了抢占安全锁(preemption-safe locks)\cite{michael1998nonblocking}，它通过确保队列中被抢占的线程不会阻止锁被授予另一个正在运行的线程，从而试图减少锁抢占对性能造成的负面影响。

许多数据结构有并发读取的需求，因此，这样的读写锁只允许线程对临界区内的数据进行读取而不能修改，如果当前临界区没有写者线程进行操作，则允许多个读者线程并发访问。
M.Crummey和T.Scott等人提出的读写队列锁算法是基于MCS队列锁并且使用读计数器和指向写者节点的指针实现的\cite{mellor1991scalable}。
Krieger等人\cite{krieger1993fair}提出了一种通过设置队列节点的双链表的队列锁，这种方法的每个节点都有自己的简单“迷你锁”，读者通过获取其相邻节点的迷你锁并重定向双链表的指针来将自己从queuelock列表中移除。

锁在并发数据结构的设计中的重要性不言而喻。
选取锁的标准一是要与应用场景结合；二是要能提供充分的扩展性。
对于哈希表这种读密集型的数据结构

\subsection{屏障技术}

屏障是这样一种机制，所有的提前执行到代码中指定的某些位置的线程悬停，只有当所有线程都到达这个点时才允许它们继续执行。 当访问数据结构或应用程序需要划分成若干个不相互重叠的执行阶段时，就需要使用内存障碍。
例如，并行垃圾收集器的标记和扫描阶段。
此外，在本文中设计的统一的并发哈希表测试框架就用到了内存屏障。
并发哈希表在测试之前需要进行初始化，也就是在哈希表内插入一些元素，让哈希表的密度达到预定的值。
初始化的过程使用多线程共同完成，由于不需要删除元素，所以这个过程在数据量大的测试集中采用多线程会节约初始化时间。
此时如果有现成提前完成了初始化的任务，它会触碰到内存屏障，必须要等待所有参与初始化的线程全部完成之后再往下执行。

实现屏障的一个简单方法是使用初始化一个值为线程总数的计数器：每个线程在到达屏障后递减计数器，然后自旋，等待计数器变为零，然后继续往下执行。
这种直观的实现方式可能引起两个方面的问题：
\begin{itemize}
	\item 当使用相同的pass/stop技术实现了多个串行屏障时，当有线程到达第二个屏障而在第一个屏障内还有一些线程没有完成时，会出现死锁；
	\item 由于所有的线程反复的查询全局变量的状态，导致通信流量大，从而对程序的可扩展性造成影响。
\end{itemize}
针对上述问题，实现了专门的屏障技术，可以让线程在不同的位置自旋\cite{brooks1986butterfly,hensgen1988two,mellor1992fast,tseng2016scalable}。
或者也可以使用Dijkstra和Scholten风格的发散计算树来实现屏障\cite{dijkstra1980termination}。在这种方法中，每个线程都是二叉树中一个节点的所有者。
线程等候它的子节点的到达，然后通知该线程的父节点以表明自己的子节点已到达。一旦所有线程都已到达，树的根节点通过向下发送释放消息释放所有线程。
除了通过软件技术实现的屏障之外，还有通过硬件实现了上述的屏障功能\cite{solihin2015fundamentals}。

\subsection{无阻塞技术}

如前文所述，使用无阻塞编程是为了克服使用锁方法带来的若干问题。
无阻塞技术包含几类条件——无等待\cite{lamport1974new,herlihy1991wait}，无锁\cite{herlihy1991wait}和无阻碍\cite{herlihy2003obstruction}。
三类条件由强到弱排列顺序依次为无等待强于无锁，无锁强于无阻碍。
但是这三类条件都强于使用诸如锁之类的阻塞结构。
虽然更强的前向条件是可取的，但是通常情况下实现较弱的保障条件更加容易、效率更高，并且易于设计和正确性验证。
所以，在实际的应用中，研究人员往往通过采取退避策略\cite{agarwal1989adaptive}或使用更复杂的竞争管理技术\cite{herlihy2003software}来补偿较弱的前向条件。

除了少数特别情况，非阻塞算法使用硬件必须提供的原子读-修改-写原语，其中最值得注意的是比较和交换指令(CAS)。
使用无阻塞方法实现的并发数据结构都是使用这些原语的标准接口实现的(在一般情况下，即使是使用了读-修改-写原子原语，临界区也是阻塞的)。
直到本文撰写，所有的非阻塞算法都必须被“原生”地写入底层原语才能达到预期的性能。

无等待算法具有最强的非阻塞前向保证条件，它保证所有CPU在连续处理有效工作时，没有运算会被其他运算所阻塞。
如果每个操作完成所需的执行步骤是有限的，则认为该算法是无等待的算法。
在性能成本不是太高的前提下这个属性对于实时系统具有非常重要的意义。
早在上个世纪九十年代，Herlihy等人就证明了所有的算法都可以实现无等待版本\cite{herlihy1988impossibility}，并且已经证明了很多被称为通用结构的串行代码转换。
但是转化后的性能与设计初衷南辕北辙。
有研究人员对实现无等待算法的难度进行了评估。
比如，文献\cite{fich2004inherent}的研究表明使用CAS、LL/SC等原子条件原语很难在不增加内存消耗和损失线性的线程扩展性的前提下实现一般的数据结构的无饥饿算法。

2011年以后，学术界和工业界对无等待算法的研究才开始重视起来。
2011年，Kogan和Petrank提出了一种基于CAS原语的无等待队列\cite{kogan2011wait}，这种无等待队列只需要普通的硬件支持即可实现。
这种无锁队列是对 Michael和Scott\cite{michael1996simple}提出的一种被广泛应用于实际的队列的扩展。
2012年Kogan和Petrank\cite{kogan2012methodology}等人又提出了一种提高无等待算法处理速度的方法并且使用这种方法实现的无等待队列的性能比无锁方法实现的相同的队列性能更好。
2014年，Timnat和Petrank\cite{timnat2014practical}提出了一种将无锁数据结构自动转化成无等待数据结构的机制。
至此，无等待实现可以用于多种数据结构中。

无锁算法允许个别线程处于饥饿状态，但能够确保系统吞吐量。
如果所有线程运行了足够长时间后，至少有一个线程能获得前向执行，那么这个算法是无锁的。
所有的无等待算法都是无锁的。
如果程序的某个或某几个线程被挂起，那么无锁算法能够保证剩下的线程能够顺利的执行。

\section{NUMA架构内存管理相关研究}

\subsection{线程与处理器内核的关联}
当前，NUMA系统上主要使用操作系统的调度程序将应用线程分配给处理器内核。 调度程序考虑系统状态和不同的策略目标（比如“平衡内核负载”或“整合内核上的线程或使内核保持为休眠状态”），然后匹配应用线程和相应的物理内核。 特定线程会在其分配的内核上执行一段时间，之后被交换到内核之外进行等待，因为其他线程也需要执行。 如果另一内核可用，调度程序将选择迁移该线程，以确保及时执行并实现其策略目标。

将线程从一个内核迁移到另一内核会导致NUMA共享内存架构出现问题，因为它会断开线程与其本地内存分配之间的关联。也就是说，线程可能启动时在节点$N_1$上分配内存，因为它运行在$N_1$的内核上。但是当该线程后来迁移至$N_2$的内核上时，之前该线程在$N_1$上保存的数据变成了远程数据，内存访问时间大幅增加。

线程与处理器内核关联。 
处理器关联指线程/进程与特定处理器资源实例相关联的持续性（无论其他实例的可用性如何）。 
通过使用系统 API，或修改操作系统数据结构（比如关联掩码），特定内核或内核集可与应用线程相关联。 然后在制定有关线程寿命的决策时，调度程序会关注这种关联方式。 
例如，线程可能配置成仅在处理器$P_1$的0-3号内核上运行。 
调度程序将在内核0-3之间进行选择，不会考虑将线程迁移至其他节点。

执行处理器关联可确保内存分配对有需要的线程保持局部性。 
不过，实行线程与处理器内核之间的关联也存在缺点。 一般来说，如果本可以使用更好的资源管理方式，处理器关联将会限制调度程序的选择，并产生资源争用现象，从而对系统性能造成不利影响。 
除了阻止调度程序将等待线程分配给未利用的内核外，处理器关联的局限性还会对应用本身产生不利影响，因为其他节点上的额外执行时间无法弥补速度较慢的内存访问。

在进行NUMA系统上的并发哈希表的设计时，必须慎重考虑处理器关联方法是否与其数据结构的特点和共享系统环境相适应。 
值得注意的是，除显式关联外，部分系统提供的处理器关联API还支持向调度程序提供优先级“提示”和关联“建议”。 相比于强制执行显示的线程绑定策略，使用此类建议能够确保在通用案例中实现最佳性能，并在高资源竞争环境下避免限制调度选择\cite{majo2017library}。

\subsection{NUMA系统的非对称互连}
\label{sec:}
NUMA架构的多处理器系统的最显著的特点就是它具有非一致的内存访问时间。
因此，线程和内存的分布对NUMA系统的性能起着至关重要的作用。
NUMA系统的这个特性在操作系统领域衍生出了许多的NUMA感知算法。
这些算法要么侧重为线程分配最近的内存节点上的内存空间\cite{brecht1993importance,lachaize2012memprof,dashti2013traffic}，或是将内存也分散在系统中以避免内存控制器和互连链路的过载\cite{dashti2013traffic}，或是将共享相同数据的线程放置在同一个内存节点上\cite{tam2007thread,tang2013optimizing}以避免内存控制器的争用\cite{bull2002data,blagodurov2010case,tang2013optimizing}，或是将可能产生缓存或内存带宽争用的线程分布在不同的内存节点上。

尽管这些上述的研究注意NUMA架构上的线程和数据的分布方式以发挥NUMA架构的性能，但是这些研究似乎都没有考虑到在未来可能会盛行的一个NUMA系统的重要属性：不对称互连。
现代操作系统旨在减少用于线程间和线程到内存通信的跳数。
运行CPU之间的负载平衡时，Linux首先使用同一节点上的CPU，然后是相隔一跳的CPU，最后是相隔两跳或更多跳数距离的CPU。
这些技术假定节点之间的互连是对称的：即对通过直接链路连接的任何节点对而言，链路具有相同的带宽和相同的等待时间。
然而在现代的NUMA系统中情况并非如此。
也就是说，当节点通过不同带宽的链路连接时，不仅要考虑线程和数据是否放置在相同或不同的节点上，而且还要考虑这些节点是如何连接的。
B.Lepers\cite{lepers2015thread}等人通过研究NUMA系统的非对称性对x86系统的影响，发现在同一个节点上的线程和数据分布相同但节点间连接不同的情况下，性能可能会相差2倍以上。
他们对于节点之间的互连有了新的认识，认为在不对称互连的特性下最好的互连方式是在总内存带宽最大的节点之间进行连接，而不是选择具有最小跳数的节点之间进行互连。
基于这个观点，他们实现了基于Linux系统的动态线程和内存分配算法\cite{lepers2015thread}。


\section{事务内存相关研究}
锁在设计并发数据结构中的关键作用是它允许线程对多个内存单元进行原子的修改，因此没有哪一个线程能够读取这些位置上的任何中间值。
事务内存机制是一种允许用户自定义的将访问多个内存单元的代码片段作为一个原子步\cite{moir2004concurrent}。
这种机制对于简化并发数据结构的设计具有重要的理论和实际意义。单纯的从算法实现上而言，编写代码时不再需要考虑哪些内存访问需要持有锁并且有效的防止了死锁问题。

用于实现并发数据结构的事务机制的灵感来源于广泛用于数据库领域的事务的概念。
虽然两者在概念上相通，但是在共享内存单元上支持事务不同于实现存储在磁盘上的数据元素的事务访问。
因此，在这种情况下，可以对于共享内存单元的事务访问的支持可以使用更加轻量级方法实现。

\subsection{实现事务内存的相关技术}
Kung和Robinson等人提出的乐观并发控制(OOC)\cite{kung1981optimistic}是一种用于实现并发数据结构的事务机制。
OOC的基本原理是在事务结束时短暂的持有全局锁。
但是使用这种方法全局锁是一个性能瓶颈，对线程扩展性造成负面影响。
理想情况下，事务性的访问的实现不应该依赖锁，并且在访问不相交的内存单元的事务之间不需要同步。

多处理器上的事务化支持首先是由Herlihy等人\cite{herlihy1993transactional}提出的，同时他们还提出了一种基于硬件的事务内存的方法。
之后，这种基于硬件的事务内存的方法被Rajwar和Goodman等人扩展到包含硬件锁省略技术\cite{rajwar2001speculative,rajwar2002transactional}。
在他们的方法中使用硬件方法将临界区自动转换为一条事务，通过这种方法使两个或多个实际上不相互冲突的临界区可以并行的执行。

事务内存发展到今天，已经有了软、硬件两种实现：基于软件的事务内存(STM)和基于硬件的事务内存(HTM)。
不论是软件事务内存\cite{spear2010lightweight,saha2006mcrt,shavit1997software,linfei2010,wangruibo2007}还是硬件事务内存\cite{yen2007logtm,moore2006logtm,dalessandro2011hybrid,wangzhaoguo2014}都得到了充分的研究与长足的发展。
随着IBM z系列~\cite{Cain2013Robust}和p系列~\cite{Wang2012Evaluation}处理器的出现标志着硬件事务内存从理论研究上升到实际研究，支持事务内存的Intel Haswell处理器~\cite{Intel2015Intel}的问世，标志着硬件事务内存实验研究走向市场化。

Intel的事务同步扩展指令集(TSX)是唯一一款支持硬件事务内存的商用处理器。
目前硬件事务内存还存在一些问题，比如Intel的Lemming效应\cite{Afek2014Software}；再比如硬件事务内存不能保证每次事务的执行都能成功提交，为了保证程序能够正确、顺利的执行，需要为程序设置回退路径处理事务不能成功提交的状况，而这个回退路径的实现往往是通过传统的锁方法实现的。
事务内存能够轻易的用于实现并发数据结构，当能够克服上述问题时，使用硬件事务内存将是设计并发数据结构的首选同步机制。

\subsection{基于事务内存的并发数据结构}

随着支持HTM的多核处理器的问世，基于HTM的并发数据结构也得到了深入的研究。
复旦大学的陈海波等实现了一系列基于HTM的并发树型数据结构\cite{wang2014using,wei2015fast,chen2016fast}。
之后，基于他们对于并发树型数据结构的研究发现由于数据冲突引起的事务中止对基于HTM的并发数据结构造成严重影响。
基于这点考虑提出了Eunomia\cite{wang2017eunomia}。
Eunomia是一种集成了若干条用于减轻事务中止的设计模式，主要用于优化搜索树这一类的数据结构。
Z.Wang等人设计了基于HTM的skip list\cite{wang2013opportunities}，通过在RTM模拟器和真实的支持RTM的处理器上对比细粒度锁、无锁方法和硬件事务内存等同步方式的比较，总结了若干条提升硬件事务内存性能的规律。
Afek\cite{Afek2014Software}等人设计了两种用于缓解Intel TSX的Lemming效应的软件优化方法，并将他们的方法与传统的CLH、MCS锁以及单纯的使用HTM的性能进行了比较。
本文基于HTM的并发哈希表的设计以及并发Cuckoo过滤器的设计都借鉴了他们提出的软件辅助方法进行优化。

\section{布隆过滤器}

在数据库、缓存、路由器和存储系统中通常需要使用判定一个元素是否存在在某个集合内，这种判断允许一定的误报率。
进行这种成员关系判定布隆过滤器(bloom filter)是使用得最多的一种数据结构\cite{bloom1970space}。
布隆过滤器最初用于拼写检查和数据库检索，随着计算机处理海量数据的压力与日俱增，布隆过滤器的研究再一次焕发新春\cite{xiekun2009}。
布隆过滤器因其高效的内存效率而备受关注。
前Google研究员吴军\cite{吴军2012数学之美}在其《数学之美》一书中指出，使用布隆过滤器的存储效率大约是使用哈希表处理同等规模数据的4到8倍。

1970年B.Bloom\cite{bloom1970space}提出了一种用于处理拼写检查的过滤器，由于其超高的空间效率和处理速度受到广泛关注，后来者为了纪念B.Bloom的突出贡献，将其创造的这种数据结构命名为Bloom过滤器。
布隆过滤器支持对元素的插入和查询操作。
它具有可变的参数误判率，记作$\epsilon$。
对元素的查询返回两种状态：一是“绝对不存在”；二是“可能存在”（这种可能存在的概率为$1-\epsilon$）。
误判率要求越低，用于表示每个元素所需的比特位越多。

为了弥补标准布隆过滤器不支持元素删除的缺陷，Counting布隆过滤器\cite{fan1998summary}对标准的布隆过滤器进行了扩展。
Counting布隆过滤器的原理很简单，就是将原来的比特数组扩展成计数器数组，当插入某元素时，将对应位置上的计数器加1，删除元素时，对应位置上的计数器减1。一般的，为了防止算数溢出计数器的大小为4比特或者4的倍数比特，所以实现Counting布隆过滤器需要至少4倍于标准布隆过滤器的存储开销。

Blocked布隆过滤器\cite{putze2007cache}重点在于优化查询效率，同样不支持删除操作。
这种过滤器是由若干个小型的布隆过滤器构成的，每个布隆过滤器的大小为一个缓存行的大小。
这一点与缓存行哈希表\cite{clht}的处理方式类似。
每进行一次查询操作，最多造成一次缓存未命中的结果，极大的提高了处理速度。
Blocked布隆过滤器的缺陷是由于各个小型布隆过滤器之间的负载不均衡造成误判率相对较高。

$d$-left Counting布隆过滤器\cite{bonomi2006improved}使用\textit{d-left hashing}\cite{mitzenmacher1999asymptotics}将元素转化为指纹信息存储在哈希表中。
删除元素时，找到对应的指纹信息进行删除。
相比于Counting布隆过滤器，它的实现更简洁，而且空间性能也得到了很大提升，空间开销只有Counting布隆过滤器的50\%，处理同等规模的数据，所需的存储空间约为标准布隆过滤器的1.5到2倍。

Quotient过滤器\cite{bender2012don}同样是一种通过哈希表存储指纹信息以实现元素删除的过滤器。
它使用类似于线性探测的方法定位目标指纹信息的位置，这种方法具有更好的空间局部性。
实现这种方法的代价是需要10\%-25\%的额外空间用于对哈希表内的实体进行编码。
此外，编码后的实体到达目标元素所在的位置时需要解码成实体序列，哈希表的密度越高，实体序列越长。
因此，这种过滤器的性能在哈希表的密度高于75\%后急剧下降。

除了上述一些具有代表性的布隆过滤器的设计方法之外，还有一些用于优化特定应用场景的布隆过滤器，如拆分型布隆过滤器\cite{xiaomingzhong2004}，基于分档布隆过滤器\cite{xiekun2007}，以及将布隆过滤器扩展到多维空间的布隆过滤器\cite{xiekun2008}等。


\section{本章小结}
并发哈希表因其对元素的查找和更新时间为常数级别的特性而被广泛应用于多核架构上的软件系统的开发。
学术界和工业界对于哈希表的研究重点也从单核处理器转移到多核处理器上。
本章的主要目的在于对与本文研究内容相关的问题的研究现状和亟待解决的问题进行归纳，以期为后面章节的研究提供理论依据。
本章首先对哈希表的基本概念进行了简单介绍，并对决定哈希表个性的哈希函数，哈希冲突处理技术进行了介绍；
然后对用于实现并发哈希表的软件同步方法进行简单的描述和比较；其次，介绍了NUMA架构与本文相关的特性——非对称互连和线程绑定；
再次，介绍了事务内存的发展，实现事务内存的软硬件技术并对事务内存在构建并发数据中的相关研究进行了归纳；
最后对与哈希表密切相关的布隆过滤器技术进行了介绍。


